# ğŸ§¬ HITnode 
> **Modular, standards-first node library for ML & GenAI pipelines.**

<p align="center">
<img src="./docs/images/logo_1_small.png" width="200">
</p>


*Author: nmc-costa*



## ğŸ“Œ Key Highlights

* **HITnode**
  Human Interface Technology-inspired, yet fully agnostic to any specific framework.

* **Standards-Driven**
  Aligns with CRISP-DM & CRISP-ML(Q) phases, LangChain patterns, Model Context Protocol (MCP) and other standards.

* **Node-Based**
  Build pipelines by snapping together reusable, well-tested nodes (`fit`/`transform`/`predict` interfaces).


---


## ğŸ¯ VISION
- Create agnostic and modular nodes (functions || classes)
- Codebase directory architecture based on standards
- Enable autonomous generation of new projects based on these nodes instead of letting GenAI build everything from scratch
- The nodes will then be used on projects to follow a node-pipeline framework
- **NOTE**: project packages source code `src/` should follow the directory structure from this codebase. 


## ğŸ”„ Node-Based Architecture

Each directory contains **nodes** - reusable components that can be combined to build ML pipelines. Nodes should follow consistent interfaces/packages/standards for easy composition and testing.

## ğŸ¯ Design Principles

- **Modularity**: Each node has a single responsibility
- **Reusability**: Nodes can be used across different pipelines
- **Clarity**: Directory and file names are self-documenting
- **Scalability**: Easy to add new nodes and extend functionality

## ğŸ› ï¸ Current Node-Pipeline Frameworks that work well with this standard
- **Native Python framework:**
  - [HICODE](https://github.com/nmc-costa/HIcode/blob/main/)
- **Custom 3rd party Framework:**
  -  [kedro](https://github.com/kedro-org/kedro)

## ğŸ“Š Methodology Standards
- **Data Mining**: [CRISP-DM](https://www.datascience-pm.com/crisp-dm-2/)
- **Machine Learning**: [CRISP-ML(Q)](https://ml-ops.org/content/crisp-ml)
- **LLM Applications**: [LangChain](https://python.langchain.com/docs/concepts/)
- **Model Integration**: [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)

## ğŸ“‹ RULES

### ğŸ¯ Node Acceptance Criteria
- **Integration**: Coordinators handle integration and merging to main branch
- **Documentation**: Nodes well documented in Jupyter notebooks with usage examples, API documentation, and integration guides
- **Testing**: Node should have tests - unit tests, dummy tests
- **Innovation**: Custom to DSML purposes or solving problems not easily handled by existing packages; 
- **Packages**: Nodes should use standard, trusted and most used packages like scikit-learn, hugging face, pandas, etc; Don't add nodes that use new and untested packages;
- **Performance**: Include benchmarks and performance considerations


### ğŸ”§ Code Standards  
- Follow [scikit-learn](https://scikit-learn.org/stable/developers/develop.html#naming), [ML-Ops](https://ml-ops.org/content/mlops-principles#naming-conventions), [CRISP-ML(Q)](https://ml-ops.org/content/crisp-ml), [LangChain](https://python.langchain.com/docs/concepts/), and [MCP](https://modelcontextprotocol.io/) conventions
- Make them as agnostic as possible to versioning: Maintain backward compatibility or provide clear migration paths
- Minimize external dependencies, use the must trusted and used packages
- Implement configuration management through `conf/` directory

### ğŸš€ Git Workflow
- Create feature branch named after the node: `feature/node-name`
- Require code review before merging to main from other coordinator
- Include performance benchmarks for computationally intensive nodes
- Run automated security scanning and compliance checks
- Map contributions to CRISP-ML(Q) phases


## ğŸš€ Getting Started
1. **Identify CRISP-ML(Q) Phase**: Determine which phase your node belongs to (Business Understanding â†’ Data Understanding â†’ Data Preparation â†’ Model Engineering â†’ Model Evaluation â†’ Model Deployment â†’ Monitoring & Maintenance)
2. **Choose Application Type**: Determine if you're building traditional ML, LLM applications, RAG systems, AI agents, or multi-modal AI
3. Choose the appropriate directory for your node type based on the phase mapping and application type
4. Follow [scikit-learn](https://scikit-learn.org/stable/developers/develop.html#naming), [ML-Ops](https://ml-ops.org/content/mlops-principles#naming-conventions), [Google ML Style Guide](https://developers.google.com/machine-learning/guides/rules-of-ml), [CRISP-ML(Q)](https://ml-ops.org/content/crisp-ml), [LangChain](https://python.langchain.com/docs/concepts/), and [MCP](https://modelcontextprotocol.io/) naming conventions
5. Implement nodes with simple packages [scikit-learn style interfaces](https://scikit-learn.org/stable/developers/develop.html#apis-of-scikit-learn-objects) (`fit`, `transform`, `predict`) and framework-specific patterns (LangChain chains, MCP tools)
6. Set up configuration management in `conf/` directory using python, yaml or JSON and following for example [Kedro patterns](https://docs.kedro.org/en/stable/configuration/configuration_basics.html)
7. Compose nodes into pipelines as needed by copying the directory into `src/`

---

# ğŸ“ Directory Structure

This codebase follows a node-based architecture where each directory contains reusable nodes that can be composed into ML pipelines, organized by **CRISP-ML(Q)** phases with modern AI/LLM integration. 

**NOTE**: 
- project packages source code `src/` should follow the directory structure from this codebase.
- If more folders are needed or redifinitions, do so on this structure

```
# Phase 1: Business & Data Understanding
business_understanding/          # Domain METADATA: Business objectives and stakeholder requirements processing into databases
â”œâ”€â”€ requirements/               # NLP nodes to extract and embed stakeholder requirements
â”œâ”€â”€ constraints/                # Parse technical/business constraints and create constraint graphs
â”œâ”€â”€ success_metrics/            # Extract and formalize business KPIs and success criteria
â”œâ”€â”€ domain_knowledge/           # Process domain-specific documents and create knowledge graphs
â””â”€â”€ context_extraction/         # Extract and structure business context for LLM applications

data_understanding/              # Data exploration and automated analysis
â”œâ”€â”€ exploratory_analysis/       # Automated EDA with statistical profiling
â”œâ”€â”€ data_quality/               # Data quality assessment and anomaly detection for text, image, audio, and video data
â”œâ”€â”€ statistics/                 # Statistical analysis and distribution modeling
â”œâ”€â”€ hypothesis_generation/      # NLP-based hypothesis generation from data insights
â””â”€â”€ metadata_extraction/        # Automated metadata extraction and cataloging

# Phase 2: Data Preparation
datasets/                        # Data I/O, ingestion, loading, and saving
â”œâ”€â”€ loaders/                    # Load data from various sources (CSV, JSON, APIs, databases) and modalities (text, image, audio, video data)
â”œâ”€â”€ savers/                     # Save data to different formats and destinations
â”œâ”€â”€ extractors/                 # Extract data from external systems and APIs
â”œâ”€â”€ validators/                 # Data quality validation and schema checking
â””â”€â”€ huggingface_datasets/       # Hugging Face dataset integration and management

preprocessing/                   # Data cleaning, transformations, feature engineering
â”œâ”€â”€ cleaners/                   # Data cleaning and quality improvement nodes
â”œâ”€â”€ transformers/               # Data type conversions and transformations
â”œâ”€â”€ feature_engineering/        # Feature creation, selection, and extraction
â”œâ”€â”€ normalizers/                # Data normalization and scaling
â”œâ”€â”€ splitters/                  # Train/validation/test data splitting
â”œâ”€â”€ text_processors/            # Text preprocessing for NLP and LLM applications
â”œâ”€â”€ tokenizers/                 # Tokenization nodes for various models and frameworks
â””â”€â”€ embeddings_prep/            # Prepare data for embedding generation

# Phase 3: Model Engineering
models/                          # Training, fitting, prediction, and inference
â”œâ”€â”€ trainers/                   # Model training and fitting nodes
â”œâ”€â”€ predictors/                 # Prediction and inference nodes
â”œâ”€â”€ tuners/                     # Hyperparameter tuning and optimization
â”œâ”€â”€ architectures/              # Model architecture definitions and configurations
â”œâ”€â”€ ensembles/                  # Ensemble methods and model combination
â”œâ”€â”€ versioning/                 # Model versioning and comparison
â”œâ”€â”€ huggingface_models/         # Hugging Face model integration and fine-tuning
â”œâ”€â”€ llm_models/                 # Large language model implementations and wrappers APIs
â”œâ”€â”€ embedding_models/           # Embedding model implementations (text, image, multimodal)
â””â”€â”€ custom_architectures/       # Custom neural network architectures

# LLM Applications
llm_applications/                # LLM-powered application components
â”œâ”€â”€ prompts/                    # Prompt engineering and template management
â”œâ”€â”€ agents/                     # AI agent implementations and workflows
â”œâ”€â”€ chains/                     # LangChain-style processing chains
â”œâ”€â”€ tools/                      # AI tools and function calling
â”œâ”€â”€ retrievers/                 # Information retrieval systems
â”œâ”€â”€ rag_systems/                # Retrieval Augmented Generation
â”œâ”€â”€ context_management/         # Context handling and memory
â””â”€â”€ mcp_integration/            # Model Context Protocol integration

# Phase 4: Model Evaluation
evaluation/                      # Unified evaluation, metrics, and analysis
â”œâ”€â”€ metrics/                    # Performance metrics (MAE, MSE, F1, AUC, BLEU, ROUGE)
â”œâ”€â”€ validation/                 # Cross-validation and model validation strategies
â”œâ”€â”€ testing/                    # A/B testing and statistical testing nodes
â”œâ”€â”€ explainability/             # Model interpretability (SHAP, LIME, feature importance)
â”œâ”€â”€ scoring/                    # Scoring predictions (thresholds, business rules)
â”œâ”€â”€ comparison/                 # Model comparison and benchmarking
â”œâ”€â”€ quality_assurance/          # Quality gates and acceptance criteria
â””â”€â”€ reports/                    # Representations, plots, demos for showing to diferent users (using the data extracted during evaluation)


# Phase 5: Model Deployment 
deployment/                      # Production deployment and serving
â”œâ”€â”€ serving/                    # Packaging, Ofuscation, containers, Docker images, serving endpoints
â”œâ”€â”€ infrastructure/             # Infrastructure as code and deployment configs
â”œâ”€â”€ user_interfaces/            # Developer, Associate, end user interfaces, StreamLit labs interfaces,
â”œâ”€â”€ pipelines/                  # Automated deployment pipelines
â”œâ”€â”€ rollback/                   # Rollback and recovery mechanisms
â”œâ”€â”€ security/                   # Security configurations and access controls
â”œâ”€â”€ agent_deployment/           # AI agent deployment systems
â””â”€â”€ edge_deployment/            # Edge and mobile deployment

# Phase 6: Monitoring & Maintenance
monitoring/                      # Continuous monitoring and maintenance
â”œâ”€â”€ performance/                # Model performance and drift monitoring
â”œâ”€â”€ data_quality/              # Ongoing data quality monitoring
â”œâ”€â”€ alerts/                    # Alerting and notification systems
â”œâ”€â”€ maintenance/               # Model maintenance and retraining triggers
â”œâ”€â”€ feedback_loops/            # Feedback collection and incorporation
â”œâ”€â”€ llm_monitoring/            # LLM-specific monitoring (token usage, latency, quality)
â””â”€â”€ agent_monitoring/          # AI agent monitoring systems

# Supporting Infrastructure
conf/                           # Configuration management and environment settings
â”œâ”€â”€ base/                      # Base configuration files
â”œâ”€â”€ local/                     # Local development overrides
â”œâ”€â”€ environments/              # Environment-specific configurations
â”œâ”€â”€ datasets/                  # data-contracts, schemas, 
â””â”€â”€ quality_gates/             # Quality assurance configurations

tests/                          # Comprehensive testing framework
â”œâ”€â”€ unit/                      # Unit tests for individual nodes
â”œâ”€â”€ integration/               # Integration tests for pipelines
â”œâ”€â”€ data_validation/           # Data quality and schema validation tests
â””â”€â”€ model_validation/          # Model performance and quality tests

storage/                        # All storage operations (RAG, vector DBs, model storage)
â”œâ”€â”€ vector_databases/          # Vector database operations for embeddings and semantic search
â”œâ”€â”€ model_registry/            # Model store versioning, storage, and retrieval
â”œâ”€â”€ embeddings/                # Store and retrieve embeddings for RAG applications
â”œâ”€â”€ feature_store/             # Feature store management and serving
â”œâ”€â”€ knowledge_graphs/          # Graph databases for constraints, relationships, domain knowledge
â””â”€â”€ document_stores/           # Document storage for RAG and knowledge systems

utils/                          # Common utilities and helper functions
â”œâ”€â”€ data_helpers/              # Data manipulation and processing utilities
â”œâ”€â”€ model_helpers/             # Model-related utility functions
â”œâ”€â”€ io_helpers/                # Input/output operation helpers
â””â”€â”€ security_helpers/          # Security and encryption utilities

docs/                           # Documentation and compliance records
â”œâ”€â”€ api/                       # API documentation
â”œâ”€â”€ tutorials/                 # Usage examples and tutorials
â”œâ”€â”€ compliance/                # Regulatory compliance documentation
â”œâ”€â”€ architecture/              # System architecture documentation
â”œâ”€â”€ methodology/               # Methodology process documentation
â””â”€â”€ stakeholder_reports/       # Reports for business stakeholders
```


---

### ğŸ“‹ GitHub Topics

```
machine-learning  crisp-ml-q  nodes  pipeline  langchain  mlops  hitnode
```
